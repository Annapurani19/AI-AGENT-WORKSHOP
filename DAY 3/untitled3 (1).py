# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mr4z5Hdk5hS4s3BASbC82DhO45m4hQ81
"""

!pip install -q sentence-transformers faiss-cpu transformers pdfplumber

import pdfplumber
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import pipeline

from google.colab import files
uploaded = files.upload()

pdf_path = list(uploaded.keys())[0]  # Get uploaded file name

def extract_text_from_pdf(pdf_path):
    all_text = ''
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            all_text += page.extract_text() + '\n'
    return all_text

document_text = extract_text_from_pdf(pdf_path)

def split_text(text, chunk_size=500, overlap=50):
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i:i+chunk_size]
        chunks.append(' '.join(chunk))
        i += chunk_size - overlap
    return chunks

chunks = split_text(document_text)

model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast model
embeddings = model.encode(chunks, show_progress_bar=True)

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

def retrieve_top_k(query, k=3):
    query_embedding = model.encode([query])
    D, I = index.search(np.array(query_embedding), k)
    return [chunks[i] for i in I[0]]

# You can try 'deepset/roberta-base-squad2' or other QA-capable models
qa_pipeline = pipeline("question-answering", model="distilbert-base-uncased-distilled-squad")

def generate_answer(query):
    context = " ".join(retrieve_top_k(query))
    result = qa_pipeline(question=query, context=context)
    return result['answer']

question = "What are the main components of a RAG model?"
answer = generate_answer(question)
print("ðŸ“Œ Question:", question)
print("âœ… Answer:", answer)

# List of your custom questions
questions = [
    "What is Retrieval-Augmented Generation (RAG)?",
    "How does the retriever component work in RAG?",
    "What is the role of the generator in a RAG model?",
    "Why is vector similarity important in RAG?",
    "What models are commonly used in RAG pipelines?",
    "How does RAG use documents from a vector store?",
    "Can RAG be used without a language model like GPT?"
]

# Loop through questions and get answers
for q in questions:
    answer = generate_answer(q)
    print(f"ðŸ“Œ Question: {q}")
    print(f"âœ… Answer: {answer}\n")